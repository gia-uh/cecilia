@article{jadhav2024limitations,
  title={On Limitations of LLM as Annotator for Low Resource Languages},
  author={Jadhav, Suramya and Shanbhag, Abhay and Thakurdesai, Amogh and Sinare, Ridhima and Joshi, Raviraj},
  journal={arXiv preprint arXiv:2411.17637},
  year={2024}
}

@article{guo2023continuous,
  title={Continuous training and fine-tuning for domain-specific language models in medical question answering},
  author={Guo, Zhen and Hua, Yining},
  journal={arXiv preprint arXiv:2311.00204},
  year={2023}
}

@article{cahyawijaya2025crowdsource,
  title={Crowdsource, crawl, or generate? creating sea-vl, a multicultural vision-language dataset for southeast asia},
  author={Cahyawijaya, Samuel and Lovenia, Holy and Moniz, Joel Ruben Antony and Wong, Tack Hwa and Farhansyah, Mohammad Rifqi and Maung, Thant Thiri and Hudi, Frederikus and Anugraha, David and Habibi, Muhammad Ravi Shulthan and Qorib, Muhammad Reza and others},
  journal={arXiv preprint arXiv:2503.07920},
  year={2025}
}

@article{vo2024redwhale,
  title={Redwhale: An adapted korean llm through efficient continual pretraining},
  author={Vo, Anh-Dung and Jung, Minseong and Lee, Wonbeen and Choi, Daewoo},
  journal={arXiv preprint arXiv:2408.11294},
  year={2024}
}

@article{gamboa2024filipino,
  title={Filipino Benchmarks for Measuring Sexist and Homophobic Bias in Multilingual Language Models from Southeast Asia},
  author={Gamboa, Lance Calvin Lim and Lee, Mark},
  journal={arXiv preprint arXiv:2412.07303},
  year={2024}
}

@article{lialin2023scaling,
  title={Scaling down to scale up: A guide to parameter-efficient fine-tuning},
  author={Lialin, Vladislav and Deshpande, Vijeta and Rumshisky, Anna},
  journal={arXiv preprint arXiv:2303.15647},
  year={2023}
}

@article{gonzalez2025salamandra,
  title={Salamandra technical report},
  author={Gonzalez-Agirre, Aitor and P{\`a}mies, Marc and Llop, Joan and Baucells, Irene and Da Dalt, Severino and Tamayo, Daniel and Saiz, Jos{\'e} Javier and Espu{\~n}a, Ferran and Prats, Jaume and Aula-Blasco, Javier and others},
  journal={arXiv preprint arXiv:2502.08489},
  year={2025}
}

@inproceedings{ogueji2021small,
  title={Small data? no problem! exploring the viability of pretrained multilingual language models for low-resourced languages},
  author={Ogueji, Kelechi and Zhu, Yuxin and Lin, Jimmy},
  booktitle={Proceedings of the 1st workshop on multilingual representation learning},
  pages={116--126},
  year={2021}
}

@article{tonja2024inkubalm,
  title={Inkubalm: A small language model for low-resource african languages},
  author={Tonja, Atnafu Lambebo and Dossou, Bonaventure FP and Ojo, Jessica and Rajab, Jenalea and Thior, Fadel and Wairagala, Eric Peter and Aremu, Anuoluwapo and Moiloa, Pelonomi and Abbott, Jade and Marivate, Vukosi and others},
  journal={arXiv preprint arXiv:2408.17024},
  year={2024}
}

@article{hu2024fox,
  title={Fox-1 Technical Report},
  author={Hu, Zijian and Zhang, Jipeng and Pan, Rui and Xu, Zhaozhuo and Avestimehr, Salman and He, Chaoyang and Zhang, Tong},
  journal={arXiv preprint arXiv:2411.05281},
  year={2024}
}

@article{ng2025sea,
  title={Sea-lion: Southeast asian languages in one network},
  author={Ng, Raymond and Nguyen, Thanh Ngan and Huang, Yuli and Tai, Ngee Chia and Leong, Wai Yi and Leong, Wei Qi and Yong, Xianbin and Ngui, Jian Gang and Susanto, Yosephine and Cheng, Nicholas and others},
  journal={arXiv preprint arXiv:2504.05747},
  year={2025}
}

@article{dong2411hymba,
  title={Hymba: A hybrid-head architecture for small language models, 2024},
  author={Dong, Xin and Fu, Yonggan and Diao, Shizhe and Byeon, Wonmin and Chen, Zijia and Mahabaleshwarkar, Ameya Sunil and Liu, Shih-Yang and Van Keirsbilck, Matthijs and Chen, Min-Hung and Suhara, Yoshi and others},
  journal={URL https://arxiv. org/abs/2411.13676}
}

@article{wu2024continual,
  title={Continual learning for large language models: A survey},
  author={Wu, Tongtong and Luo, Linhao and Li, Yuan-Fang and Pan, Shirui and Vu, Thuy-Trang and Haffari, Gholamreza},
  journal={arXiv preprint arXiv:2402.01364},
  year={2024}
}

@article{ishibashi2025mining,
  title={Mining hidden thoughts from texts: Evaluating continual pretraining with synthetic data for llm reasoning},
  author={Ishibashi, Yoichi and Yano, Taro and Oyamada, Masafumi},
  journal={arXiv preprint arXiv:2505.10182},
  year={2025}
}

@inproceedings{yan2023af,
  title={Af adapter: Continual pretraining for building chinese biomedical language model},
  author={Yan, Yongyu and Xue, Kui and Shi, Xiaoming and Ye, Qi and Liu, Jingping and Ruan, Tong},
  booktitle={2023 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)},
  pages={953--957},
  year={2023},
  organization={IEEE}
}

@article{yao2021adapt,
  title={Adapt-and-distill: Developing small, fast and effective pretrained language models for domains},
  author={Yao, Yunzhi and Huang, Shaohan and Wang, Wenhui and Dong, Li and Wei, Furu},
  journal={arXiv preprint arXiv:2106.13474},
  year={2021}
}

@article{shu2024transcending,
  title={Transcending language boundaries: Harnessing llms for low-resource language translation},
  author={Shu, Peng and Chen, Junhao and Liu, Zhengliang and Wang, Hui and Wu, Zihao and Zhong, Tianyang and Li, Yiwei and Zhao, Huaqin and Jiang, Hanqi and Pan, Yi and others},
  journal={arXiv preprint arXiv:2411.11295},
  year={2024}
}

@article{chavan2023one,
  title={One-for-all: Generalized lora for parameter-efficient fine-tuning},
  author={Chavan, Arnav and Liu, Zhuang and Gupta, Deepak and Xing, Eric and Shen, Zhiqiang},
  journal={arXiv preprint arXiv:2306.07967},
  year={2023}
}

@article{goyal2022flores,
  title={The flores-101 evaluation benchmark for low-resource and multilingual machine translation},
  author={Goyal, Naman and Gao, Cynthia and Chaudhary, Vishrav and Chen, Peng-Jen and Wenzek, Guillaume and Ju, Da and Krishnan, Sanjana and Ranzato, Marc’Aurelio and Guzm{\'a}n, Francisco and Fan, Angela},
  journal={Transactions of the Association for Computational Linguistics},
  volume={10},
  pages={522--538},
  year={2022},
  publisher={MIT Press One Broadway, 12th Floor, Cambridge, Massachusetts 02142, USA~…}
}

@article{hasan2021xl,
  title={XL-sum: Large-scale multilingual abstractive summarization for 44 languages},
  author={Hasan, Tahmid and Bhattacharjee, Abhik and Islam, Md Saiful and Samin, Kazi and Li, Yuan-Fang and Kang, Yong-Bin and Rahman, M Sohel and Shahriyar, Rifat},
  journal={arXiv preprint arXiv:2106.13822},
  year={2021}
}

@article{joshi2017triviaqa,
  title={Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension},
  author={Joshi, Mandar and Choi, Eunsol and Weld, Daniel S and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:1705.03551},
  year={2017}
}

@article{artetxe2019cross,
  title={On the cross-lingual transferability of monolingual representations},
  author={Artetxe, Mikel and Ruder, Sebastian and Yogatama, Dani},
  journal={arXiv preprint arXiv:1910.11856},
  year={2019}
}

@article{bandarkar2023belebele,
  title={The belebele benchmark: a parallel reading comprehension dataset in 122 language variants},
  author={Bandarkar, Lucas and Liang, Davis and Muller, Benjamin and Artetxe, Mikel and Shukla, Satya Narayan and Husa, Donald and Goyal, Naman and Krishnan, Abhinandan and Zettlemoyer, Luke and Khabsa, Madian},
  journal={arXiv preprint arXiv:2308.16884},
  year={2023}
}

@article{conneau2018xnli,
  title={XNLI: Evaluating cross-lingual sentence representations},
  author={Conneau, Alexis and Lample, Guillaume and Rinott, Ruty and Williams, Adina and Bowman, Samuel R and Schwenk, Holger and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1809.05053},
  year={2018}
}

@inproceedings{lin2022few,
  title={Few-shot learning with multilingual generative language models},
  author={Lin, Xi Victoria and Mihaylov, Todor and Artetxe, Mikel and Wang, Tianlu and Chen, Shuohui and Simig, Daniel and Ott, Myle and Goyal, Naman and Bhosale, Shruti and Du, Jingfei and others},
  booktitle={Proceedings of the 2022 conference on empirical methods in natural language processing},
  pages={9019--9052},
  year={2022}
}

@article{jayawardena2024parafusion,
  title={Parafusion: A large-scale llm-driven english paraphrase dataset infused with high-quality lexical and syntactic diversity},
  author={Jayawardena, Lasal and Yapa, Prasan},
  journal={arXiv preprint arXiv:2404.12010},
  year={2024}
}

@article{mihaylov2018can,
  title={Can a suit of armor conduct electricity? a new dataset for open book question answering},
  author={Mihaylov, Todor and Clark, Peter and Khot, Tushar and Sabharwal, Ashish},
  journal={arXiv preprint arXiv:1809.02789},
  year={2018}
}

@article{clark2018think,
  title={Think you have solved question answering? try arc, the ai2 reasoning challenge},
  author={Clark, Peter and Cowhey, Isaac and Etzioni, Oren and Khot, Tushar and Sabharwal, Ashish and Schoenick, Carissa and Tafjord, Oyvind},
  journal={arXiv preprint arXiv:1803.05457},
  year={2018}
}

@article{biderman2024lessons,
  title={Lessons from the trenches on reproducible evaluation of language models},
  author={Biderman, Stella and Schoelkopf, Hailey and Sutawika, Lintang and Gao, Leo and Tow, Jonathan and Abbasi, Baber and Aji, Alham Fikri and Ammanamanchi, Pawan Sasanka and Black, Sidney and Clive, Jordan and others},
  journal={arXiv preprint arXiv:2405.14782},
  year={2024}
}

@article{ellison2025lora,
  title={LoRA-Based Lightweight Adaptation of Pretrained Models for Low-Resource Text Summarization},
  author={Ellison, Thayer},
  journal={Journal of Computer Science and Software Applications},
  volume={5},
  number={6},
  year={2025}
}

@article{chen2025lora,
  title={CE-LoRA: Computation-Efficient LoRA Fine-Tuning for Language Models},
  author={Chen, Guanduo and He, Yutong and Hu, Yipeng and Yuan, Kun and Yuan, Binhang},
  journal={arXiv preprint arXiv:2502.01378},
  year={2025}
}