@article{jadhav2024limitations,
  title={On Limitations of LLM as Annotator for Low Resource Languages},
  author={Jadhav, Suramya and Shanbhag, Abhay and Thakurdesai, Amogh and Sinare, Ridhima and Joshi, Raviraj},
  journal={arXiv preprint arXiv:2411.17637},
  year={2024}
}

@article{guo2023continuous,
  title={Continuous training and fine-tuning for domain-specific language models in medical question answering},
  author={Guo, Zhen and Hua, Yining},
  journal={arXiv preprint arXiv:2311.00204},
  year={2023}
}

@article{cahyawijaya2025crowdsource,
  title={Crowdsource, crawl, or generate? creating sea-vl, a multicultural vision-language dataset for southeast asia},
  author={Cahyawijaya, Samuel and Lovenia, Holy and Moniz, Joel Ruben Antony and Wong, Tack Hwa and Farhansyah, Mohammad Rifqi and Maung, Thant Thiri and Hudi, Frederikus and Anugraha, David and Habibi, Muhammad Ravi Shulthan and Qorib, Muhammad Reza and others},
  journal={arXiv preprint arXiv:2503.07920},
  year={2025}
}

@article{vo2024redwhale,
  title={Redwhale: An adapted korean llm through efficient continual pretraining},
  author={Vo, Anh-Dung and Jung, Minseong and Lee, Wonbeen and Choi, Daewoo},
  journal={arXiv preprint arXiv:2408.11294},
  year={2024}
}

@article{gamboa2024filipino,
  title={Filipino Benchmarks for Measuring Sexist and Homophobic Bias in Multilingual Language Models from Southeast Asia},
  author={Gamboa, Lance Calvin Lim and Lee, Mark},
  journal={arXiv preprint arXiv:2412.07303},
  year={2024}
}

@article{lialin2023scaling,
  title={Scaling down to scale up: A guide to parameter-efficient fine-tuning},
  author={Lialin, Vladislav and Deshpande, Vijeta and Rumshisky, Anna},
  journal={arXiv preprint arXiv:2303.15647},
  year={2023}
}

@article{gonzalez2025salamandra,
  title={Salamandra technical report},
  author={Gonzalez-Agirre, Aitor and P{\`a}mies, Marc and Llop, Joan and Baucells, Irene and Da Dalt, Severino and Tamayo, Daniel and Saiz, Jos{\'e} Javier and Espu{\~n}a, Ferran and Prats, Jaume and Aula-Blasco, Javier and others},
  journal={arXiv preprint arXiv:2502.08489},
  year={2025}
}

@inproceedings{ogueji2021small,
  title={Small data? no problem! exploring the viability of pretrained multilingual language models for low-resourced languages},
  author={Ogueji, Kelechi and Zhu, Yuxin and Lin, Jimmy},
  booktitle={Proceedings of the 1st workshop on multilingual representation learning},
  pages={116--126},
  year={2021}
}

@article{tonja2024inkubalm,
  title={Inkubalm: A small language model for low-resource african languages},
  author={Tonja, Atnafu Lambebo and Dossou, Bonaventure FP and Ojo, Jessica and Rajab, Jenalea and Thior, Fadel and Wairagala, Eric Peter and Aremu, Anuoluwapo and Moiloa, Pelonomi and Abbott, Jade and Marivate, Vukosi and others},
  journal={arXiv preprint arXiv:2408.17024},
  year={2024}
}

@article{hu2024fox,
  title={Fox-1 Technical Report},
  author={Hu, Zijian and Zhang, Jipeng and Pan, Rui and Xu, Zhaozhuo and Avestimehr, Salman and He, Chaoyang and Zhang, Tong},
  journal={arXiv preprint arXiv:2411.05281},
  year={2024}
}

@article{ng2025sea,
  title={Sea-lion: Southeast asian languages in one network},
  author={Ng, Raymond and Nguyen, Thanh Ngan and Huang, Yuli and Tai, Ngee Chia and Leong, Wai Yi and Leong, Wei Qi and Yong, Xianbin and Ngui, Jian Gang and Susanto, Yosephine and Cheng, Nicholas and others},
  journal={arXiv preprint arXiv:2504.05747},
  year={2025}
}

@article{dong2411hymba,
  title={Hymba: A hybrid-head architecture for small language models, 2024},
  author={Dong, Xin and Fu, Yonggan and Diao, Shizhe and Byeon, Wonmin and Chen, Zijia and Mahabaleshwarkar, Ameya Sunil and Liu, Shih-Yang and Van Keirsbilck, Matthijs and Chen, Min-Hung and Suhara, Yoshi and others},
  journal={URL https://arxiv. org/abs/2411.13676}
}

@article{wu2024continual,
  title={Continual learning for large language models: A survey},
  author={Wu, Tongtong and Luo, Linhao and Li, Yuan-Fang and Pan, Shirui and Vu, Thuy-Trang and Haffari, Gholamreza},
  journal={arXiv preprint arXiv:2402.01364},
  year={2024}
}

@article{ishibashi2025mining,
  title={Mining hidden thoughts from texts: Evaluating continual pretraining with synthetic data for llm reasoning},
  author={Ishibashi, Yoichi and Yano, Taro and Oyamada, Masafumi},
  journal={arXiv preprint arXiv:2505.10182},
  year={2025}
}

@inproceedings{yan2023af,
  title={Af adapter: Continual pretraining for building chinese biomedical language model},
  author={Yan, Yongyu and Xue, Kui and Shi, Xiaoming and Ye, Qi and Liu, Jingping and Ruan, Tong},
  booktitle={2023 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)},
  pages={953--957},
  year={2023},
  organization={IEEE}
}

@article{yao2021adapt,
  title={Adapt-and-distill: Developing small, fast and effective pretrained language models for domains},
  author={Yao, Yunzhi and Huang, Shaohan and Wang, Wenhui and Dong, Li and Wei, Furu},
  journal={arXiv preprint arXiv:2106.13474},
  year={2021}
}

@article{shu2024transcending,
  title={Transcending language boundaries: Harnessing llms for low-resource language translation},
  author={Shu, Peng and Chen, Junhao and Liu, Zhengliang and Wang, Hui and Wu, Zihao and Zhong, Tianyang and Li, Yiwei and Zhao, Huaqin and Jiang, Hanqi and Pan, Yi and others},
  journal={arXiv preprint arXiv:2411.11295},
  year={2024}
}

@article{chavan2023one,
  title={One-for-all: Generalized lora for parameter-efficient fine-tuning},
  author={Chavan, Arnav and Liu, Zhuang and Gupta, Deepak and Xing, Eric and Shen, Zhiqiang},
  journal={arXiv preprint arXiv:2306.07967},
  year={2023}
}








