@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{su2024roformer,
  title={Roformer: Enhanced transformer with rotary position embedding},
  author={Su, Jianlin and Ahmed, Murtadha and Lu, Yu and Pan, Shengfeng and Bo, Wen and Liu, Yunfeng},
  journal={Neurocomputing},
  volume={568},
  pages={127063},
  year={2024},
  publisher={Elsevier}
}

@article{gonzalez2025salamandra,
  title={Salamandra technical report},
  author={Gonzalez-Agirre, Aitor and P{\`a}mies, Marc and Llop, Joan and Baucells, Irene and Da Dalt, Severino and Tamayo, Daniel and Saiz, Jos{\'e} Javier and Espu{\~n}a, Ferran and Prats, Jaume and Aula-Blasco, Javier and others},
  journal={arXiv preprint arXiv:2502.08489},
  year={2025}
}

@article{ke2023continual,
  title={Continual pre-training of language models},
  author={Ke, Zixuan and Shao, Yijia and Lin, Haowei and Konishi, Tatsuya and Kim, Gyuhak and Liu, Bing},
  journal={arXiv preprint arXiv:2302.03241},
  year={2023}
}

@inproceedings{rust2021good,
  title={How Good is Your Tokenizer? On the Monolingual Performance of Multilingual Language Models},
  author={Rust, Phillip and Pfeiffer, Jonas and Vuli{\'c}, Ivan and Gurevych, Iryna and Ruder, Sebastian},
  booktitle={Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics (ACL)},
  pages={3118--3135},
  year={2021}
}

@inproceedings{gururangan2020dont,
  title={Don't Stop Pretraining: Adapt Language Models to Domains and Tasks},
  author={Gururangan, Suchin and Marasovi{\'c}, Ana and Swayamdipta, Swabha and Lo, Kyle and Beltagy, Iz and Downey, Doug and Smith, Noah A.},
  booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  pages={8342--8360},
  year={2020}
}

@article{bommasani2021opportunities,
  title={On the Opportunities and Risks of Foundation Models},
  author={Bommasani, Rishi and Hudson, Drew A. and Adeli, Ehsan and Altman, Russ and Arora, Simran and von Arx, Sydney and others},
  journal={arXiv preprint arXiv:2108.07258},
  year={2021}
}

@article{bender2021dangers,
  title={On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?},
  author={Bender, Emily M. and Gebru, Timnit and McMillan-Major, Angelina and Shmitchell, Shmargaret},
  journal={Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
  pages={610--623},
  year={2021}
}

@inproceedings{blodgett2020language,
  title={Language (Technology) is Power: A Critical Survey of “Bias” in NLP},
  author={Blodgett, Su Lin and Barocas, Solon and Daum{\'e} III, Hal and Wallach, Hanna},
  booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  pages={5454--5476},
  year={2020}
}

@inproceedings{arc,
  title={Think you have solved question answering? Try ARC, the AI2 Reasoning Challenge},
  author={Clark, Peter and Cowhey, Isaac and Etzioni, Oren and Khot, Tushar and Sabharwal, Ashish and Schoenick, Carissa and Tafjord, Oyvind},
  booktitle={Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages={47--54},
  year={2018}
}

@inproceedings{openbookqa,
  title={OpenBookQA: A New Benchmark for Open Book Question Answering},
  author={Mihaylov, Todor and Clark, Peter and Khot, Tushar and Sabharwal, Ashish},
  booktitle={Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
  pages={2381--2391},
  year={2018}
}

@inproceedings{paws,
  title={PAWS: Paraphrase Adversaries from Word Scrambling},
  author={Yang, Yinfei and Zhang, Yuan and Tar, Chris and Baldridge, Jason},
  booktitle={Proceedings of NAACL-HLT},
  year={2019}
}

@inproceedings{piqa,
  title={PIQA: Reasoning about Physical Commonsense in Natural Language},
  author={Bisk, Yonatan and Zellers, Rowan and Bras, Ronan Le and Gao, Jianfeng and Choi, Yejin},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={34},
  number={05},
  pages={7432--7439},
  year={2020}
}

@inproceedings{socialiqa,
  title={SocialIQA: Commonsense Reasoning about Social Interactions},
  author={Sap, Maarten and Le Bras, Ronan and Allaway, Emily and Bhagavatula, Chandra and Lourie, Nicholas and Rashkin, Hannah and Roof, Brendan and Smith, Noah A and Choi, Yejin},
  booktitle={Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing},
  pages={4463--4473},
  year={2019}
}

@inproceedings{teca,
  title={TECA: A Textual Entailment Corpus for the Spanish Language},
  author={Rodríguez, Elena and Gutiérrez, Yoan and Almeida, Yudivián and Montoyo, Andrés and Muñoz, Rafael and Palomar, Manuel},
  booktitle={Proceedings of the 12th International Conference on Language Resources and Evaluation (LREC 2020)},
  year={2020}
}

@inproceedings{wnli,
  title={The Winograd Schema Challenge},
  author={Levesque, Hector and Davis, Ernest and Morgenstern, Leora},
  booktitle={Proceedings of the Thirteenth International Conference on Principles of Knowledge Representation and Reasoning},
  pages={552--561},
  year={2012}
}

@inproceedings{xnli,
  title={XNLI: Evaluating Cross-lingual Sentence Representations},
  author={Conneau, Alexis and Rinott, Ruty and Lample, Guillaume and Williams, Adina and Bowman, Samuel and Schwenk, Holger and Stoyanov, Veselin},
  booktitle={Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
  pages={2475--2485},
  year={2018}
}

@inproceedings{xstorycloze,
  title={The Story Cloze Test: A Platform for Evaluating Machine Story Understanding},
  author={Mostafazadeh, Nasrin and Chambers, Nathanael and He, Xiaodong and Parikh, Ankur and Batra, Dhruv and Vanderwende, Lucy and Kohli, Pushmeet and Allen, James},
  booktitle={Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages={280--290},
  year={2016}
}

@inproceedings{belebele,
  title={Belebele: A Novel Multilingual Reading Comprehension Dataset for Low-Resource Languages},
  author={Ahuja, Kabir and Adewumi, Tosin and Adelani, David Ifeoluwa and others},
  booktitle={arXiv preprint arXiv:2307.09641},
  year={2023}
}

@inproceedings{xlsum,
  title={XL-Sum: Large-Scale Multilingual Abstractive Summarization for 44 Languages},
  author={Hasan, Tahmid and Bhattacharjee, Abhik and Islam, Md Saiful and Mubarak, Hamdy and Abdelali, Ahmed and Sajjad, Hassan and Durrani, Nadir},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2021},
  pages={4693--4703},
  year={2021}
}

@inproceedings{triviaqa,
  title={TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension},
  author={Joshi, Mandar and Choi, Eunsol and Weld, Daniel and Zettlemoyer, Luke},
  booktitle={Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={1601--1611},
  year={2017}
}

@inproceedings{xquad,
  title={XQuAD: A Cross-lingual Question Answering Dataset},
  author={Artetxe, Mikel and Ruder, Sebastian and Yogatama, Dani},
  booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages={8658--8663},
  year={2020}
}

@article{goodfellow2013empirical,
  title={An Empirical Investigation of Catastrophic Forgetting in Gradient-Based Neural Networks},
  author={Goodfellow, Ian J and Mirza, Mehdi and Xiao, Da and Courville, Aaron and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1312.6211},
  year={2013}
}

@article{li2024culturellm,
  title={Culturellm: Incorporating cultural differences into large language models},
  author={Li, Cheng and Chen, Mengzhuo and Wang, Jindong and Sitaram, Sunayana and Xie, Xing},
  journal={Advances in Neural Information Processing Systems},
  volume={37},
  pages={84799--84838},
  year={2024}
}

@article{patil2025regional,
  title={Regional Tiny Stories: Using Small Models to Compare Language Learning and Tokenizer Performance},
  author={Patil, Nirvan and Inamdar, Malhar Abhay and Gosai, Agnivo and Pathak, Guruprasad and Joshi, Anish and Sagavekar, Aryan and Joshirao, Anish and Dandekar, Raj and Dandekar, Rajat and Panat, Sreedath},
  journal={arXiv preprint arXiv:2504.07989},
  year={2025}
}

@article{li2023tinystories,
  title={TinyStories: How small can language models be and still speak coherent English},
  author={Li, Yuanzhi and Eldan, Ronen},
  year={2023}
}

@article{gururangan2020don,
  title={Don't stop pretraining: Adapt language models to domains and tasks},
  author={Gururangan, Suchin and Marasovi{\'c}, Ana and Swayamdipta, Swabha and Lo, Kyle and Beltagy, Iz and Downey, Doug and Smith, Noah A},
  journal={arXiv preprint arXiv:2004.10964},
  year={2020}
}

@article{zhang2019root,
  title={Root mean square layer normalization},
  author={Zhang, Biao and Sennrich, Rico},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@article{yildiz2024investigating,
  title={Investigating continual pretraining in large language models: Insights and implications},
  author={Y{\i}ld{\i}z, {\c{C}}a{\u{g}}atay and Ravichandran, Nishaanth Kanna and Sharma, Nitin and Bethge, Matthias and Ermis, Beyza},
  journal={arXiv preprint arXiv:2402.17400},
  year={2024}
}

@article{lee2024impact,
  title={The impact of model size on catastrophic forgetting in Online Continual Learning},
  author={Lee, Eunhae},
  journal={arXiv preprint arXiv:2407.00176},
  year={2024}
}

@article{vsliogeris2025full,
  title={Full-Parameter Continual Pretraining of Gemma2: Insights into Fluency and Domain Knowledge},
  author={{\v{S}}liogeris, Vytenis and Daniu{\v{s}}is, Povilas and Nakvosas, Arturas},
  journal={arXiv preprint arXiv:2505.05946},
  year={2025}
}

@article{tellez2023regionalized,
  title={Regionalized models for Spanish language variations based on Twitter},
  author={Tellez, Eric S and Moctezuma, Daniela and Miranda, Sabino and Graff, Mario and Ruiz, Guillermo},
  journal={Language Resources and Evaluation},
  volume={57},
  number={4},
  pages={1697--1727},
  year={2023},
  publisher={Springer}
}

@article{liu2023dada,
  title={Dada: Dialect adaptation via dynamic aggregation of linguistic rules},
  author={Liu, Yanchen and Held, William and Yang, Diyi},
  journal={arXiv preprint arXiv:2305.13406},
  year={2023}
}
