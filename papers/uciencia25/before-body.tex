% Render the title
\author{Yudivi√°n Almeida-Cruz\inst{1}\orcidID{0000-1111-2222-3333} \and
Suilan Estevez-Velarde\inst{1}\orcidID{1111-2222-3333-4444} \and
Alejandro Piad-Morffis\inst{1}\orcidID{2222--3333-4444-5555}}
%

% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
%
\institute{University of Havana, Cuba \and
University of Alicante, Spain}

\maketitle

% Render the abstract if present
\begin{abstract}
Large language models have transformed natural language processing, but their effectiveness is limited for regional language variants due to underrepresentation in training data. This paper introduces Cecilia 2B, a 2-billion-parameter language model continually pretrained on nearly 1 billion tokens of Cuban Spanish text, including newspapers, encyclopedias, legal documents, literature, and song lyrics, to address the gap in language technology for Cuban Spanish. Leveraging the Salamandra 2B architecture, Cecilia 2B demonstrates the feasibility and value of adapting multilingual models to regional variants through continual pretraining, achieving improved performance on Spanish and multilingual tasks relevant to its target domain while maintaining computational efficiency suitable for resource-constrained environments. We detail the construction of a culturally rich Cuban Spanish corpus, the adaptation methodology, and a comparative evaluation with the base model, highlighting both the benefits and trade-offs of regional specialization. Cecilia 2B provides a foundational resource for Cuban Spanish NLP and establishes a path for future research in instruction tuning, corpus expansion, tokenizer retraining, and the development of larger or more specialized models.

\keywords{Language Modeling \and Natural Language Processing \and Corpora.}
\end{abstract}
